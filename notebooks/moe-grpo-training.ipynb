{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee08852",
   "metadata": {},
   "source": [
    "# MoE Training with GRPO\n",
    "\n",
    "### WandB Configuration\n",
    "\n",
    "In this step, we set up **Weights & Biases (WandB)** to track and log our GRPO training experiments.  \n",
    "WandB allows us to monitor key metrics, store hyperparameters, and compare runs for reproducibility.\n",
    "\n",
    "Specifically, we:\n",
    "\n",
    "- Set WandB environment variables including the API key and logging directory.  \n",
    "- Define the main hyperparameters for **GRPO training**: learning rate, batch size, number of rollouts, buffer size, and number of epochs.  \n",
    "- Initialize a new WandB run with these hyperparameters for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfa_mekrache\u001b[0m (\u001b[33maasr\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../wandb/run-20250907_233347-muqlk6j2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aasr/blue-yonder-mle-assignment/runs/muqlk6j2' target=\"_blank\">granite-3.0-grpo-3</a></strong> to <a href='https://wandb.ai/aasr/blue-yonder-mle-assignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aasr/blue-yonder-mle-assignment' target=\"_blank\">https://wandb.ai/aasr/blue-yonder-mle-assignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aasr/blue-yonder-mle-assignment/runs/muqlk6j2' target=\"_blank\">https://wandb.ai/aasr/blue-yonder-mle-assignment/runs/muqlk6j2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aasr/blue-yonder-mle-assignment/runs/muqlk6j2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ec9575d2750>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os \n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"<your_api_key_here>\"\n",
    "os.environ[\"WANDB_DIR\"] = \"..\"\n",
    "\n",
    "learning_rate = 1e-6\n",
    "batch_size = 1\n",
    "num_rollouts = 4\n",
    "buffer_size = 40\n",
    "num_epochs = 2\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"blue-yonder-mle-assignment\", \n",
    "    name=\"granite-3.0-grpo-3\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_rollouts\": num_rollouts,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aabbd2",
   "metadata": {},
   "source": [
    "### Model and LoRA Configuration\n",
    "\n",
    "In this step, we load the **fine-tuned SFT model** (LoRA adapters already trained) and prepare it for **GRPO training**.\n",
    "\n",
    "Specifically, we:\n",
    "\n",
    "- Load the **base IBM Granite MoE model** using `transformers`.  \n",
    "- Load the previously fine-tuned **SFT LoRA checkpoint** via PEFT.  \n",
    "- Initialize the tokenizer from the SFT checkpoint, ensuring `padding_side=\"left\"` and aligning the pad token with the EOS token.  \n",
    "- Ensure that **LoRA parameters remain trainable** for GRPO updates.  \n",
    "- Set up the optimizer and prepare the model, tokenizer, and optimizer with **`Accelerator`** for efficient multi-device training.\n",
    "\n",
    "This setup allows the model to start GRPO from the **SFT fine-tuned weights**, leveraging the previously learned reasoning capabilities while enabling policy optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baea60e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arora/mekrache/moe-reasoning/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,752,512 || all params: 1,337,377,792 || trainable%: 0.2058\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "\n",
    "base_model_id = \"ibm-granite/granite-3.0-1b-a400m-base\"\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load SFT adapter\n",
    "llm = PeftModel.from_pretrained(model, \"../checkpoints/granite-1b-a400m-blue-yonder-sft/checkpoint-402\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../checkpoints/granite-1b-a400m-blue-yonder-sft/checkpoint-402\",  local_files_only=True)\n",
    "\n",
    "# Ensure LoRA parameters are trainable\n",
    "for name, param in llm.named_parameters():\n",
    "    if 'lora' in name.lower():\n",
    "        param.requires_grad = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Print trainable parameters\n",
    "print(llm.print_trainable_parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(llm.parameters(), lr=learning_rate)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "llm, tokenizer, optimizer = accelerator.prepare(\n",
    "    llm, tokenizer, optimizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d526e1",
   "metadata": {},
   "source": [
    "### Expert Utilization Monitoring\n",
    "\n",
    "This section monitors **how frequently each expert is used** in the MoE layers during training.  \n",
    "\n",
    "- A **hook function** is registered on the main MoE layers and their gates to track the outputs of the gating mechanism.  \n",
    "- During each forward pass, the hook accumulates the usage of each expert in a global dictionary.  \n",
    "- The `log_expert_usage` function computes the **percentage of usage per expert** and clears the buffer for the next step.  \n",
    "\n",
    "This monitoring helps analyze **load balancing and expert activity** throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b06f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 main MoE layers: ['model.layers.0.block_sparse_moe', 'model.layers.1.block_sparse_moe', 'model.layers.2.block_sparse_moe', 'model.layers.3.block_sparse_moe', 'model.layers.4.block_sparse_moe', 'model.layers.5.block_sparse_moe', 'model.layers.6.block_sparse_moe', 'model.layers.7.block_sparse_moe', 'model.layers.8.block_sparse_moe', 'model.layers.9.block_sparse_moe', 'model.layers.10.block_sparse_moe', 'model.layers.11.block_sparse_moe', 'model.layers.12.block_sparse_moe', 'model.layers.13.block_sparse_moe', 'model.layers.14.block_sparse_moe', 'model.layers.15.block_sparse_moe', 'model.layers.16.block_sparse_moe', 'model.layers.17.block_sparse_moe', 'model.layers.18.block_sparse_moe', 'model.layers.19.block_sparse_moe', 'model.layers.20.block_sparse_moe', 'model.layers.21.block_sparse_moe', 'model.layers.22.block_sparse_moe', 'model.layers.23.block_sparse_moe']\n"
     ]
    }
   ],
   "source": [
    "# Expert Utilization Monitoring\n",
    "expert_usage = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        expert_weights = output[1]  \n",
    "        if hasattr(expert_weights, 'sum') and len(expert_weights.shape) == 2:\n",
    "            usage_per_expert = expert_weights.sum(dim=0).detach().cpu().numpy()\n",
    "            for i, usage in enumerate(usage_per_expert):\n",
    "                if i not in expert_usage:\n",
    "                    expert_usage[i] = 0.0\n",
    "                expert_usage[i] += float(usage)\n",
    "\n",
    "# Find only main MoE layers and register hooks\n",
    "moe_layers = []\n",
    "for name, module in model.named_modules():\n",
    "    if name.endswith('block_sparse_moe'):\n",
    "        moe_layers.append((name, module))\n",
    "        module.register_forward_hook(hook_fn)\n",
    "        if hasattr(module, 'gate'):\n",
    "            module.gate.register_forward_hook(hook_fn)\n",
    "\n",
    "print(f\"Found {len(moe_layers)} main MoE layers: {[n for n,m in moe_layers]}\")\n",
    "\n",
    "def log_expert_usage():\n",
    "    \"\"\"\n",
    "    Returns a dictionary with flattened expert usage percentages.\n",
    "\n",
    "    Args:\n",
    "        expert_usage (dict): Dictionary with expert counts, e.g., {'0': 5, '1': 3, ...}\n",
    "\n",
    "    Returns:\n",
    "        dict: Flattened expert usage percentages, e.g.,\n",
    "              {'expert_usage/expert_0': 25.0, 'expert_usage/expert_1': 15.0, ...}\n",
    "    \"\"\"\n",
    "    logs = {}\n",
    "    if expert_usage:\n",
    "        total = sum(expert_usage.values())\n",
    "        if total > 0:\n",
    "            expert_percent = {f\"expert_usage/expert_{k}\": float(v) / total * 100 for k, v in expert_usage.items()}\n",
    "        else:\n",
    "            expert_percent = {f\"expert_usage/expert_{k}\": 0.0 for k in expert_usage}\n",
    "\n",
    "        logs.update(expert_percent)\n",
    "        expert_usage.clear()  # optional: clear buffer after computing\n",
    "\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26006af9",
   "metadata": {},
   "source": [
    "### GRPO Training Loop\n",
    "\n",
    "This section runs the **GRPO training loop** on the GSM8K dataset using the SFT fine-tuned LoRA model.\n",
    "\n",
    "Key steps:\n",
    "\n",
    "1. **Environment Setup:**  \n",
    "   - Load GSM8K dataset and initialize a custom RL environment (`GSM8KEnv`) that handles batching and reward computation.\n",
    "\n",
    "2. **Experience Collection:**  \n",
    "   - Sample a batch of problems from the environment each iteration.  \n",
    "   - Generate model outputs and compute **rewards** using the environment.  \n",
    "   - Accumulate experiences in a buffer for training.  \n",
    "   - Track **expert usage** via `log_expert_usage`.\n",
    "\n",
    "3. **Policy Update (GRPO):**  \n",
    "   - When the buffer reaches the defined size:\n",
    "     - Shuffle and split it into mini-batches.  \n",
    "     - For each mini-batch and epoch, compute loss with `train_on_batch`.  \n",
    "     - Update gradients using the optimizer and `Accelerator` for multi-device support.  \n",
    "   - Compute the **average loss** over all updates.\n",
    "\n",
    "4. **Reward Function:**  \n",
    "   - Combines multiple aspects to guide policy updates:\n",
    "     1. **Correctness of the final answer (70%)** – reward 1.0 if correct, else 0.0.  \n",
    "     2. **Formatting (15%)** – ensures model outputs both reasoning (`<think>`) and final answer (`<answer>`).  \n",
    "     3. **Reasoning quality (15%)** – compares model’s reasoning to reference using **BERTScore F1**.  \n",
    "\n",
    "5. **Logging and Monitoring:**  \n",
    "   - Log **mean reward**, **average loss**, and **expert usage** to WandB at each step.  \n",
    "   - Print sample metrics to monitor training progress.\n",
    "\n",
    "6. **Termination and Saving:**  \n",
    "   - Stop training after reaching the maximum number of steps.  \n",
    "   - Save the updated LoRA model and tokenizer to the checkpoint directory.\n",
    "\n",
    "This loop allows the model to **improve its policy** via GRPO while leveraging reasoning capabilities learned during SFT, guided by a structured reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73394179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: Loss = 0.0292, Mean Reward = 0.2355\n",
      "Step 19: Loss = 0.0333, Mean Reward = 0.9578\n",
      "Step 29: Loss = 0.0228, Mean Reward = 0.2527\n",
      "Step 39: Loss = 0.0250, Mean Reward = 0.4478\n",
      "Step 49: Loss = 0.0357, Mean Reward = 0.4267\n",
      "Step 59: Loss = 0.0186, Mean Reward = 0.4287\n",
      "Step 69: Loss = 0.0442, Mean Reward = 0.4516\n",
      "Step 79: Loss = 0.0244, Mean Reward = 0.2512\n",
      "Step 90: Loss = 0.0189, Mean Reward = 0.6125\n",
      "Step 100: Loss = 0.0334, Mean Reward = 0.7571\n",
      "Step 110: Loss = 0.0316, Mean Reward = 0.6240\n",
      "Step 120: Loss = 0.0363, Mean Reward = 0.2482\n",
      "Step 130: Loss = 0.0221, Mean Reward = 0.8029\n",
      "Step 140: Loss = 0.0353, Mean Reward = 0.7986\n",
      "Step 151: Loss = 0.0317, Mean Reward = 0.6253\n",
      "Step 161: Loss = 0.0228, Mean Reward = 0.9678\n",
      "Step 171: Loss = 0.0200, Mean Reward = 0.2489\n",
      "Step 181: Loss = 0.0407, Mean Reward = 0.2450\n",
      "Step 192: Loss = 0.0403, Mean Reward = 0.4341\n",
      "Step 202: Loss = 0.0442, Mean Reward = 0.2454\n",
      "Step 212: Loss = 0.0262, Mean Reward = 0.2512\n",
      "Step 222: Loss = 0.0203, Mean Reward = 0.2525\n",
      "Step 232: Loss = 0.0452, Mean Reward = 0.2395\n",
      "Step 242: Loss = 0.0275, Mean Reward = 0.7973\n",
      "Step 252: Loss = 0.0321, Mean Reward = 0.6243\n",
      "Step 262: Loss = 0.0304, Mean Reward = 0.7973\n",
      "Step 272: Loss = 0.0244, Mean Reward = 0.4108\n",
      "Step 282: Loss = 0.0253, Mean Reward = 0.9717\n",
      "Step 293: Loss = 0.0315, Mean Reward = 0.2436\n",
      "Step 303: Loss = 0.0321, Mean Reward = 0.2512\n",
      "Step 313: Loss = 0.0354, Mean Reward = 0.8149\n",
      "Step 323: Loss = 0.0352, Mean Reward = 0.2425\n",
      "Step 334: Loss = 0.0304, Mean Reward = 0.6111\n",
      "Step 344: Loss = 0.0404, Mean Reward = 0.4150\n",
      "Step 354: Loss = 0.0329, Mean Reward = 0.7869\n",
      "Step 365: Loss = 0.0388, Mean Reward = 0.2589\n",
      "Step 375: Loss = 0.0311, Mean Reward = 0.2180\n",
      "Step 385: Loss = 0.0360, Mean Reward = 0.5945\n",
      "Step 395: Loss = 0.0316, Mean Reward = 0.6216\n",
      "Step 405: Loss = 0.0328, Mean Reward = 0.9774\n",
      "Step 415: Loss = 0.0257, Mean Reward = 0.2528\n",
      "Step 425: Loss = 0.0534, Mean Reward = 0.2518\n",
      "Step 435: Loss = 0.0204, Mean Reward = 0.2344\n",
      "Step 445: Loss = 0.0363, Mean Reward = 0.4242\n",
      "Step 455: Loss = 0.0298, Mean Reward = 0.8030\n",
      "Step 465: Loss = 0.0357, Mean Reward = 0.2227\n",
      "Step 475: Loss = 0.0310, Mean Reward = 0.7957\n",
      "Step 485: Loss = 0.0434, Mean Reward = 0.2585\n",
      "Step 495: Loss = 0.0289, Mean Reward = 0.1675\n",
      "Reached maximum training steps. Exiting training loop.\n",
      "Total training time: 01:21:25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../checkpoints/granite-1b-a400m-blue-yonder-grpo-3/tokenizer_config.json',\n",
       " '../checkpoints/granite-1b-a400m-blue-yonder-grpo-3/special_tokens_map.json',\n",
       " '../checkpoints/granite-1b-a400m-blue-yonder-grpo-3/vocab.json',\n",
       " '../checkpoints/granite-1b-a400m-blue-yonder-grpo-3/merges.txt',\n",
       " '../checkpoints/granite-1b-a400m-blue-yonder-grpo-3/added_tokens.json',\n",
       " '../checkpoints/granite-1b-a400m-blue-yonder-grpo-3/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from datasets import load_dataset\n",
    "sys.path.append('..')\n",
    "from utils.dataset import GSM8KEnv\n",
    "from utils.grpo import collect_experiences, train_on_batch\n",
    "import random\n",
    "import time\n",
    "\n",
    "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "gsm8k_train_env = GSM8KEnv(gsm8k_dataset, tokenizer)\n",
    "gsm8k_train_env.reset()\n",
    "printing_steps = 50\n",
    "\n",
    "buffer = []\n",
    "step = 0\n",
    "training_steps = 500\n",
    "batch_mean_reward = 0.0\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "while True:  \n",
    "    try:\n",
    "        # Sample batch from environment\n",
    "        batch = gsm8k_train_env.sample_batch(batch_size)\n",
    "\n",
    "        experiences, mean_reward = collect_experiences(llm, tokenizer, accelerator, batch, batch_size, num_rollouts)\n",
    "        buffer.extend(experiences)\n",
    "        \n",
    "        # Expert usage\n",
    "        logs = {\n",
    "            \"rl_training/mean_reward\": mean_reward,\n",
    "            **log_expert_usage(),\n",
    "        }\n",
    "\n",
    "        if len(buffer) >= buffer_size:  \n",
    "            buffer = buffer[-buffer_size:]  # Keep only the most recent experiences\n",
    "\n",
    "            random.shuffle(buffer)\n",
    "\n",
    "            buffer = buffer[:buffer_size]  \n",
    "            optimizer.zero_grad()\n",
    "            llm.train()\n",
    "\n",
    "            total_loss = 0.0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for _ in range(num_epochs):\n",
    "                for i in range(0, buffer_size, batch_size):\n",
    "                    \n",
    "                    training_batch = buffer[i : i + batch_size]\n",
    "                    loss = train_on_batch(llm, tokenizer, accelerator, training_batch)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    accelerator.backward(loss)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    num_batches += 1\n",
    "            \n",
    "            avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "            # Prepare logs for loss and expert usage\n",
    "            logs[\"rl_training/avg_loss\"] = avg_loss\n",
    "\n",
    "            print(f\"Step {step}: Loss = {avg_loss:.4f}, Mean Reward = {mean_reward:.4f}\")\n",
    "\n",
    "            buffer = []\n",
    "\n",
    "        # Logging\n",
    "        wandb.log(logs, step=step)\n",
    "        step += 1\n",
    "\n",
    "        if step >= training_steps:\n",
    "            print(\"Reached maximum training steps. Exiting training loop.\")\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at step {step}: {e}\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        break\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print summary\n",
    "hours, rem = divmod(elapsed_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(f\"Total training time: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
    "\n",
    "output_dir = \"../checkpoints/granite-1b-a400m-blue-yonder-grpo-3\"\n",
    "llm.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69b4c8",
   "metadata": {},
   "source": [
    "## Evaluation on GSM8K Test Set\n",
    "\n",
    "We evaluate the GRPO-trained model on the **GSM8K test set** using a custom environment (`GSM8KEnv`) that handles problem presentation, step-by-step reasoning, and scoring.\n",
    "\n",
    "**Evaluation loop:**\n",
    "1. Reset the environment to get a new math problem.  \n",
    "2. Apply the chat template to format the input.  \n",
    "3. Generate reasoning and answer with the model.  \n",
    "4. Decode the output and submit it to the environment to receive a reward.  \n",
    "5. Accumulate the total score to compute a weighted accuracy.\n",
    "\n",
    "The **reward function** considers not only whether the final answer is correct but also the **format and reasoning quality**, combining three components:\n",
    "\n",
    "1. **Correctness of the final answer (70%)**  \n",
    "   - 1.0 if the predicted answer matches the gold answer exactly, else 0.0.\n",
    "\n",
    "2. **Formatting reward (15%)**  \n",
    "   - Checks that the model produced both reasoning (`<think>`) and an answer (`<answer>`).  \n",
    "   - Encourages structured and readable outputs.\n",
    "\n",
    "3. **Reasoning similarity (15%)**  \n",
    "   - Measures how close the model's reasoning is to the reference using **BERTScore F1**.  \n",
    "   - Encourages coherent, step-by-step explanations.\n",
    "\n",
    "The **weighted sum** of these three metrics forms the final reward for each sample, providing a comprehensive evaluation of reasoning and answer quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f38ed32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 11/1319 [00:48<1:44:36,  4.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1319/1319 [1:23:20<00:00,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on 1319 samples:\n",
      "  Weighted score : 43.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "eval_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "gsm8k_eval_env = GSM8KEnv(eval_dataset, tokenizer)\n",
    "\n",
    "gsm8k_eval_env.current_idx = 0\n",
    "total_score = 0.0\n",
    "llm.eval()\n",
    "\n",
    "for i in tqdm(range(len(gsm8k_eval_env.dataset)), desc=\"Evaluating\"):\n",
    "    # Get problem from environment\n",
    "    obs, _ = gsm8k_eval_env.reset() \n",
    "\n",
    "    text = obs\n",
    "\n",
    "    output_ids = llm.generate(\n",
    "        **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "\n",
    "    pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    _, reward, terminated, truncated, info = gsm8k_eval_env.step(pred)  # Gymnasium returns 5 values\n",
    "    gold = info['gold']\n",
    "    \n",
    "    total_score += reward\n",
    "\n",
    "N = len(gsm8k_eval_env.dataset)\n",
    "print(f\"\\nResults on {N} samples:\")\n",
    "print(f\"  Weighted score : {total_score / N:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
