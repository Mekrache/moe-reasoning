{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80313d4c",
   "metadata": {},
   "source": [
    "# MoE Supervised Fine-Tuning with LoRA\n",
    "\n",
    "### WandB Configuration\n",
    "\n",
    "We start by configuring [Weights & Biases (WandB)](https://wandb.ai/) for experiment tracking.  \n",
    "This allows us to log key metrics and hyperparameters during training for better monitoring and reproducibility.  \n",
    "\n",
    "In this step, we:  \n",
    "- Set the WandB environment variables (API key, project, run name, and logging directory).  \n",
    "- Define the main hyperparameters for **Supervised Fine-Tuning (SFT)**: learning rate, batch size, number of epochs, and maximum sequence length.  \n",
    "- Specify the base **Mixture-of-Experts (MoE)** model ID from IBM Granite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d39ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"<your_api_key_here>\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"blue-yonder-mle-assignment\"\n",
    "os.environ[\"WANDB_RUN_NAME\"] = \"granite-3.0-sft\"\n",
    "os.environ[\"WANDB_DIR\"] = \"..\"\n",
    "\n",
    "learning_rate = 3e-4\n",
    "batch_size = 3\n",
    "num_train_epochs = 2\n",
    "max_seq_length = 1024\n",
    "base_model_id = \"ibm-granite/granite-3.0-1b-a400m-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abab7f8",
   "metadata": {},
   "source": [
    "### Model and LoRA Configuration\n",
    "\n",
    "Next, we load the **base IBM Granite MoE model** and prepare it for supervised fine-tuning with **LoRA (Low-Rank Adaptation)**.  \n",
    "\n",
    "In this step, we:  \n",
    "- Load the pre-trained causal language model with `transformers`.  \n",
    "- Initialize the corresponding tokenizer, setting `padding_side=\"left\"` for causal models and aligning the pad token with the EOS token.  \n",
    "- Define the **LoRA configuration** to inject trainable low-rank adapters into key projection layers.  \n",
    "- Wrap the model with PEFTâ€™s `get_peft_model` to apply LoRA.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5cd38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arora/mekrache/moe-reasoning/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",    \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d1629",
   "metadata": {},
   "source": [
    "After applying **LoRA**, only a small fraction of the modelâ€™s parameters are trainable, which significantly reduces the computational and memory cost compared to full fine-tuning.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e410be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,752,512 || all params: 1,337,377,792 || trainable%: 0.2058\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f68c57",
   "metadata": {},
   "source": [
    "### SFT Chat Template with Reasoning and Answer Tags\n",
    "\n",
    "When performing **Supervised Fine-Tuning (SFT)** on chat-style data, it is important to provide the model with a consistent structure for inputs and outputs.  \n",
    "We define a **chat template** that:  \n",
    "\n",
    "- Wraps internal reasoning inside `<think>...</think>`.  \n",
    "- Places the final answer inside `<answer>...</answer>`.  \n",
    "\n",
    "This improves the **readability of the outputs**, as reasoning and answers are clearly separated.  \n",
    "It also helps the model learn a structured format that can later be used consistently during inference.  \n",
    "\n",
    "Additionally, the chat template enables the model to behave like a **chat assistant** after SFT, handling multi-turn interactions in a natural way.  \n",
    "\n",
    "ðŸ“– For more details, see the [Hugging Face TRL SFT Trainer documentation](https://huggingface.co/docs/trl/en/sft_trainer).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408a1369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|endoftext|><|user|>\\nWhat is 1+1?\\n<|assistant|>\\n<think>I think it's 2.</think><answer>2</answer><|endoftext|>\\n<|user|>\\nWhat is 2+2?\\n<|assistant|><think>\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_start = \"<think>\" \n",
    "reasoning_end   = \"</think>\"   \n",
    "solution_start  = \"<answer>\"\n",
    "solution_end    = \"</answer>\"\n",
    "\n",
    "chat_template = \\\n",
    "    \"{{ bos_token }}\"\\\n",
    "    \"{% for message in messages %}\"\\\n",
    "        \"{% if message['role'] == 'system' %}\"\\\n",
    "            \"{{ '<|system|>\\n' + message['content'] + '\\n' }}\"\\\n",
    "        \"{% elif message['role'] == 'user' %}\"\\\n",
    "            \"{{ '<|user|>\\n' + message['content'] + '\\n' }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{% if not loop.last %}\"\\\n",
    "                \"{{ '<|assistant|>\\n' + message['content'] + eos_token + '\\n' }}\"\\\n",
    "            \"{% else %}\"\\\n",
    "                \"{{ '<|assistant|>\\n' + message['content'] + eos_token }}\"\\\n",
    "            \"{% endif %}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "        \"{% if loop.last and add_generation_prompt %}\"\\\n",
    "            \"{{ '<|assistant|>' }}{{ '{reasoning_start}' }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\n",
    "\n",
    "# Replace with specific template:\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "tokenizer.apply_chat_template([\n",
    "    #{\"role\" : \"system\", \"content\" : \"You are given a math problem. You must first think step by step and then give the final answer.\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
    "    {\"role\" : \"assistant\", \"content\" : f\"{reasoning_start}I think it's 2.{reasoning_end}{solution_start}2{solution_end}\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"},\n",
    "], tokenize = False, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a279c",
   "metadata": {},
   "source": [
    "We load the **GSM8K** dataset and convert each example into a `messages` list that matches our SFT chat template.  \n",
    "For GSM8K the `answer` field contains a chain-of-thought (rationale) followed by the final solution separated by `####`.  \n",
    "We **split** on that delimiter, wrap the reasoning in `<think>...</think>` and the final answer in `<answer>...</answer>`, and store the formatted conversation as the `messages` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e5acf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'You are given a math problem. You must first think step by step and then give the final answer.',\n",
       "  'role': 'system'},\n",
       " {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "  'role': 'user'},\n",
       " {'content': '<think>Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.</think><answer>72</answer>',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split = \"train\")\n",
    "dataset = dataset.to_pandas()[\n",
    "    [\"question\", \"answer\"]\n",
    "]\n",
    "\n",
    "def format_dataset(x):\n",
    "    answer = x[\"answer\"]\n",
    "    question = x[\"question\"]\n",
    "\n",
    "    # Split thoughts and answer\n",
    "    thoughts, final_answer = answer.split(\"####\")\n",
    "\n",
    "    final_answer = final_answer.strip()\n",
    "    thoughts = thoughts.strip()\n",
    "    \n",
    "    # Add our custom formatting\n",
    "    final_prompt = \\\n",
    "        reasoning_start + thoughts + reasoning_end + \\\n",
    "        solution_start + final_answer + solution_end\n",
    "    return [\n",
    "        {\"role\" : \"system\", \"content\" : \"You are given a math problem. You must first think step by step and then give the final answer.\"},\n",
    "        {\"role\" : \"user\",      \"content\" : question},\n",
    "        {\"role\" : \"assistant\", \"content\" : final_prompt},\n",
    "    ]\n",
    "\n",
    "dataset[\"messages\"] = dataset.apply(format_dataset, axis = 1)\n",
    "\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "dataset[0][\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6854cf91",
   "metadata": {},
   "source": [
    "### Testing the Base Model on Reasoning\n",
    "\n",
    "Before fine-tuning, itâ€™s useful to **observe the performance of the base model** on the reasoning task.  \n",
    "Here, we feed a single example from the GSM8K dataset and prompt the model to generate its reasoning and final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629b9fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><|system|>\n",
      "You are given a math problem. You must first think step by step and then give the final answer.\n",
      "<|user|>\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "<|assistant|><think>\n",
      "To find the total number of clips Natalia sold, we need to calculate the number of clips she sold in April and May separately and then add them together.\n",
      "\n",
      "In April, Natalia sold 48 clips.\n",
      "\n",
      "In May, Natalia sold half as many clips as she did in April, which is 48/2 = 24 clips.\n",
      "\n",
      "To find the total number of clips Natalia sold in April and May, we add the number of clips she sold in each month: 48 + 24 = 72 clips.\n",
      "\n",
      "Therefore, Natalia sold a total of 72 clips in April and May.\n",
      "\n",
      "<|user|>\n",
      "<|assistant|><answer>\n",
      "72\n",
      "<|user|>\n",
      "<|assistant|><solution>\n",
      "72\n",
      "<|user|>\n",
      "<|assistant|><explanation>\n",
      "Natalia sold 48 clips in April and then sold half as many clips in May, which is 48/2 = 24 clips. To find the total number of clips she sold in April and May, we add the number of\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    dataset[0][\"messages\"][:2],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca31361",
   "metadata": {},
   "source": [
    "### Training Setup with TRL SFTTrainer\n",
    "\n",
    "We configure **supervised fine-tuning (SFT)** using the [TRL SFTTrainer](https://huggingface.co/docs/trl/en/sft_trainer), which handles batching, optimization, logging, and checkpointing for large language models.\n",
    "\n",
    "Here, we define the training configuration (`SFTConfig`) with key settings such as gradient checkpointing, batch size, sequence length, number of epochs, learning rate, optimizer, and logging.  \n",
    "\n",
    "We then initialize the trainer with our **LoRA-adapted MoE model**, the tokenizer, and the prepared dataset.\n",
    "\n",
    "Once the `SFTTrainer` is configured, we can begin training the model. This step updates only the **LoRA adapter weights**, keeping the base model frozen, and logs training progress to WandB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce44794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arora/mekrache/moe-reasoning/.env/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfa_mekrache\u001b[0m (\u001b[33maasr\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../wandb/run-20250906_114056-6wz2wa5l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aasr/blue-yonder-mle-assignment/runs/6wz2wa5l' target=\"_blank\">../checkpoints/granite-1b-a400m-blue-yonder-sft</a></strong> to <a href='https://wandb.ai/aasr/blue-yonder-mle-assignment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aasr/blue-yonder-mle-assignment' target=\"_blank\">https://wandb.ai/aasr/blue-yonder-mle-assignment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aasr/blue-yonder-mle-assignment/runs/6wz2wa5l' target=\"_blank\">https://wandb.ai/aasr/blue-yonder-mle-assignment/runs/6wz2wa5l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='402' max='402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [402/402 38:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.412900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.336200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.352200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.350800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.338700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.330100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.318200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.324600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.314800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.321700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.312400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.326300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.323100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.319900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.337200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.315300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.317900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=402, training_loss=0.3448613013201092, metrics={'train_runtime': 2320.7155, 'train_samples_per_second': 1.554, 'train_steps_per_second': 0.173, 'total_flos': 2.8514847100502016e+16, 'train_loss': 0.3448613013201092, 'epoch': 2.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    gradient_checkpointing=True,   \n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False}, \n",
    "    gradient_accumulation_steps=1,  \n",
    "    per_device_train_batch_size=16, \n",
    "    auto_find_batch_size=True,\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=True,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    optim='paged_adamw_8bit',           \n",
    "    logging_steps=10,\n",
    "    output_dir='../checkpoints/granite-1b-a400m-blue-yonder-sft',\n",
    "    report_to='wandb',\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b0777",
   "metadata": {},
   "source": [
    "### Testing the Fine-Tuned Model on Reasoning\n",
    "\n",
    "After supervised fine-tuning, we test the modelâ€™s ability to generate **step-by-step reasoning** followed by the **final answer** on a GSM8K example.  \n",
    "\n",
    "We use the same chat template with `<think>` and `<answer>` tags to format the input, and `TextStreamer` to stream the output in real time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5d7519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|><|system|>\n",
      "You are given a math problem. You must first think step by step and then give the final answer.\n",
      "<|user|>\n",
      "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "<|assistant|><think>In April, Natalia sold 48 clips.\n",
      "In May, she sold half as many clips as in April, which is 48/2 = <<48/2=24>>24 clips.\n",
      "In total, Natalia sold 48 + 24 = <<48+24=72>>72 clips in April and May.</think><answer>72</answer><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    dataset[0][\"messages\"][:2],\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "text\n",
    "\n",
    "from transformers import TextStreamer\n",
    "output = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 256,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e4959",
   "metadata": {},
   "source": [
    "## Evaluation on GSM8K Test Set\n",
    "\n",
    "We evaluate the fine-tuned model on the **GSM8K test set** using a custom environment (`GSM8KEnv`) that handles problem presentation, step-by-step reasoning, and scoring.\n",
    "\n",
    "The evaluation loop:\n",
    "1. Resets the environment to get a new math problem.\n",
    "2. Applies the chat template to format the input.\n",
    "3. Generates reasoning and answer with the model.\n",
    "4. Decodes the output and submits it to the environment to receive a reward.\n",
    "5. Accumulates the total score to compute a weighted accuracy.\n",
    "\n",
    "\n",
    "The **reward function** considers not only whether the final answer is correct but also the **format and reasoning quality**. The reward combines three components:\n",
    "\n",
    "1. Correctness of the final answer (70% weight)  \n",
    "   - 1.0 if the predicted answer matches the gold answer exactly, else 0.0.  \n",
    "\n",
    "2. Formatting reward (15% weight)  \n",
    "   - Ensures that the model produced both reasoning (`<think>`) and an answer (`<answer>`).  \n",
    "   - Encourages structured outputs for readability and consistency.  \n",
    "\n",
    "3. Reasoning similarity (15% weight)  \n",
    "   - Measures how close the model's reasoning is to the reference using **BERTScore F1**.  \n",
    "   - Encourages coherent, step-by-step explanations.\n",
    "\n",
    "The **weighted sum** of these three metrics forms the final reward for each sample.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12406a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 3/1319 [00:06<44:58,  2.05s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1319/1319 [1:14:25<00:00,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results on 1319 samples:\n",
      "  Weighted score : 39.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from utils.dataset import GSM8KEnv\n",
    "\n",
    "eval_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "gsm8k_eval_env = GSM8KEnv(eval_dataset, tokenizer)\n",
    "\n",
    "gsm8k_eval_env.current_idx = 0\n",
    "total_score = 0.0\n",
    "model.eval()\n",
    "\n",
    "for i in tqdm(range(len(gsm8k_eval_env.dataset)), desc=\"Evaluating\"):\n",
    "    # Get problem from environment\n",
    "    obs, _ = gsm8k_eval_env.reset() \n",
    "\n",
    "    text = obs\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    _, reward, terminated, truncated, info = gsm8k_eval_env.step(pred)  # Gymnasium returns 5 values\n",
    "    gold = info['gold']\n",
    "\n",
    "    total_score += reward\n",
    "\n",
    "N = len(gsm8k_eval_env.dataset)\n",
    "print(f\"\\nResults on {N} samples:\")\n",
    "print(f\"  Weighted score : {total_score / N:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
